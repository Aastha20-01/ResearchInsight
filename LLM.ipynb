{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "hLCSUi3exg2D",
        "outputId": "8917d6b2-b909-4b2f-ee4e-40e746d25e49"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzqWNhwyyQQy"
      },
      "outputs": [],
      "source": [
        "pdf_folder = \"/content/drive/MyDrive/Colab Notebooks/pdf2\"\n",
        "text_folder = \"/content/drive/MyDrive/txt1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1aPWpE_zCkW"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xt6SBrtg3S9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Folder containing PDFs\n",
        "pdf_folder = \"/content/drive/MyDrive/Colab Notebooks/pdf2\"\n",
        "# Folder to save text files\n",
        "text_folder = \"texts\"\n",
        "\n",
        "# Create output folder if it doesn't exist\n",
        "os.makedirs(text_folder, exist_ok=True)\n",
        "\n",
        "# Loop over all PDF files in the folder\n",
        "for filename in os.listdir(pdf_folder):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_folder, filename)\n",
        "        text_path = os.path.join(text_folder, filename.replace(\".pdf\", \".txt\"))\n",
        "\n",
        "        # Open and extract text\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "\n",
        "        # Write text to file\n",
        "        with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "\n",
        "print(\" Conversion completed. Text files are saved in 'texts' folder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq-9vN-9BVXZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83d97ead"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "text_folder = \"texts\"\n",
        "text_files = [os.path.join(text_folder, f) for f in os.listdir(text_folder) if f.endswith(\".txt\")]\n",
        "\n",
        "all_text = \"\"\n",
        "for file_path in text_files:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        all_text += f.read()\n",
        "\n",
        "print(all_text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9a1253e"
      },
      "outputs": [],
      "source": [
        "chunk_size = 1000\n",
        "text_chunks = [all_text[i:i+chunk_size] for i in range(0, len(all_text), chunk_size)]\n",
        "\n",
        "print(f\"Total number of chunks: {len(text_chunks)}\")\n",
        "print(\"First chunk:\")\n",
        "print(text_chunks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "063e6bc5"
      },
      "source": [
        "## Choose and load an llm model\n",
        "\n",
        "### Subtask:\n",
        "Select a suitable LLM model from Hugging Face that can be fine-tuned for summarization and text generation. Load the model and its tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a63fd707"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"meta-llama/Llama-3-8b\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model '{model_name}' and tokenizer loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "995aa504"
      },
      "source": [
        "## Fine-tune the llm model\n",
        "\n",
        "### Subtask:\n",
        "Train the chosen LLM model on the prepared text data. This step will involve defining the training parameters, setting up the training loop, and monitoring the training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73f0d3d3"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "\n",
        "# Add a padding token if the tokenizer doesn't have one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize the text chunks\n",
        "tokenized_chunks = tokenizer(text_chunks, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Create a Hugging Face Dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    \"input_ids\": tokenized_chunks[\"input_ids\"],\n",
        "    \"attention_mask\": tokenized_chunks[\"attention_mask\"],\n",
        "})\n",
        "\n",
        "# Create a data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "print(\"Text data tokenized and dataset created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88d6afba"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Output directory\n",
        "    num_train_epochs=10,  # Number of training epochs\n",
        "    per_device_train_batch_size=10,  # Batch size per device during training\n",
        "    save_steps=10_000,  # Save model every 10,000 steps\n",
        "    save_total_limit=2,  # Only keep the last 2 checkpoints\n",
        "    logging_dir=\"./logs\",  # Directory for storing logs\n",
        "    logging_steps=100, # Log every 200 steps\n",
        ")\n",
        "\n",
        "# Instantiate Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Training arguments defined and Trainer instantiated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "306363fb"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a390db5e"
      },
      "outputs": [],
      "source": [
        "def generate_summary_and_abstract(prompt, model, tokenizer, max_length=512, num_return_sequences=1, no_repeat_ngram_size=2, do_sample=True, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generates a summary and abstract from a user prompt using a fine-tuned LLM.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The user prompt.\n",
        "        model: The fine-tuned Hugging Face model.\n",
        "        tokenizer: The corresponding tokenizer.\n",
        "        max_length (int): The maximum length of the generated text.\n",
        "        num_return_sequences (int): The number of sequences to generate.\n",
        "        no_repeat_ngram_size (int): The size of ngrams that should not be repeated.\n",
        "        do_sample (bool): Whether to use sampling for generation.\n",
        "        temperature (float): Controls the randomness in sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text containing the summary and abstract.\n",
        "    \"\"\"\n",
        "    # Tokenize the input prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=True)\n",
        "\n",
        "    # Generate text\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.eos_token_id # Use eos_token_id as pad_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaa1dbcc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_summary_abstract_and_references(generated_text, original_chunks):\n",
        "    \"\"\"\n",
        "    Extracts summary and abstract from generated text and finds relevant references.\n",
        "\n",
        "    Args:\n",
        "        generated_text (str): The text generated by the LLM.\n",
        "        original_chunks (list): A list of original text chunks.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the extracted summary (str), abstract (str),\n",
        "               and relevant references (list of str).\n",
        "    \"\"\"\n",
        "    summary = \"\"\n",
        "    abstract = \"\"\n",
        "    references = []\n",
        "\n",
        "\n",
        "    summary_match = re.search(r\"Summary:\\s*(.*?)(?:Abstract:|$)\", generated_text, re.DOTALL | re.IGNORECASE)\n",
        "    if summary_match:\n",
        "        summary = summary_match.group(1).strip()\n",
        "\n",
        "    abstract_match = re.search(r\"Abstract:\\s*(.*?)(?:Summary:|$)\", generated_text, re.DOTALL | re.IGNORECASE)\n",
        "    if abstract_match:\n",
        "        abstract = abstract_match.group(1).strip()\n",
        "\n",
        "    if not summary and not abstract:\n",
        "        # Simple split, might need adjustment based on actual output format\n",
        "        split_text = generated_text.split('\\n\\n', 1)\n",
        "        abstract = split_text[0].strip()\n",
        "        if len(split_text) > 1:\n",
        "            summary = split_text[1].strip()\n",
        "        else:\n",
        "            summary = abstract \n",
        "\n",
        "    keywords = set()\n",
        "    keywords.update(summary.split()[:10])\n",
        "    keywords.update(abstract.split()[:10])\n",
        "\n",
        "\n",
        "    relevant_chunks = []\n",
        "    for i, chunk in enumerate(original_chunks):\n",
        "        if any(re.search(r'\\b' + re.escape(keyword) + r'\\b', chunk, re.IGNORECASE) for keyword in keywords if len(keyword) > 3): # Check for keywords with length > 3\n",
        "            relevant_chunks.append(f\"Reference from chunk {i+1}: ...{chunk[:200]}...\") # Include snippet and chunk number\n",
        "\n",
        "    references = relevant_chunks\n",
        "\n",
        "    return summary, abstract, references\n",
        "\n",
        "def generate_summary_abstract_and_references_with_prompt(prompt, model, tokenizer, original_chunks, max_length=512, num_return_sequences=1, no_repeat_ngram_size=2, do_sample=True, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generates summary, abstract, and references for a given prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The user prompt.\n",
        "        model: The fine-tuned Hugging Face model.\n",
        "        tokenizer: The corresponding tokenizer.\n",
        "        original_chunks (list): A list of original text chunks.\n",
        "        max_length (int): The maximum length of the generated text.\n",
        "        num_return_sequences (int): The number of sequences to generate.\n",
        "        no_repeat_ngram_size (int): The size of ngrams that should not be repeated.\n",
        "        do_sample (bool): Whether to use sampling for generation.\n",
        "        temperature (float): Controls the randomness in sampling.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the extracted summary (str), abstract (str),\n",
        "               and relevant references (list of str).\n",
        "    \"\"\"\n",
        "    generated_text = generate_summary_and_abstract(\n",
        "        prompt,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    summary, abstract, references = extract_summary_abstract_and_references(generated_text, original_chunks)\n",
        "\n",
        "    return summary, abstract, references\n",
        "\n",
        "# Example usage:\n",
        "user_prompt = \"Summarize the key findings about data analysis in the provided documents.\"\n",
        "summary, abstract, references = generate_summary_abstract_and_references_with_prompt(user_prompt, model, tokenizer, text_chunks)\n",
        "\n",
        "print(\"--- Generated Summary ---\")\n",
        "print(summary)\n",
        "print(\"\\n--- Generated Abstract ---\")\n",
        "print(abstract)\n",
        "print(\"\\n--- Relevant References ---\")\n",
        "if references:\n",
        "    for ref in references:\n",
        "        print(ref)\n",
        "else:\n",
        "    print(\"No relevant references found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSCumgpcD63U"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Choose the same model used before\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model '{model_name}' and tokenizer re-loaded successfully.\")\n",
        "\n",
        "# Re-run the example usage from the previous cell\n",
        "user_prompt = \"Summarize the key findings about data analysis in the provided documents.\"\n",
        "summary, abstract, references = generate_summary_abstract_and_references_with_prompt(user_prompt, model, tokenizer, text_chunks)\n",
        "\n",
        "print(\"--- Generated Summary ---\")\n",
        "print(summary)\n",
        "print(\"\\n--- Generated Abstract ---\")\n",
        "print(abstract)\n",
        "print(\"\\n--- Relevant References ---\")\n",
        "if references:\n",
        "    for ref in references:\n",
        "        print(ref)\n",
        "else:\n",
        "    print(\"No relevant references found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCMhxCXwEMee"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "text_folder = \"texts\"\n",
        "text_files = [os.path.join(text_folder, f) for f in os.listdir(text_folder) if f.endswith(\".txt\")]\n",
        "\n",
        "all_text = \"\"\n",
        "for file_path in text_files:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        all_text += f.read()\n",
        "\n",
        "# Re-create text chunks\n",
        "chunk_size = 1000\n",
        "text_chunks = [all_text[i:i+chunk_size] for i in range(0, len(all_text), chunk_size)]\n",
        "\n",
        "print(f\"Total number of chunks: {len(text_chunks)}\")\n",
        "print(\"Text chunks re-created.\")\n",
        "\n",
        "user_prompt = \"Summarize the key findings about data analysis in the provided documents.\"\n",
        "summary, abstract, references = generate_summary_abstract_and_references_with_prompt(user_prompt, model, tokenizer, text_chunks)\n",
        "\n",
        "print(\"--- Generated Summary ---\")\n",
        "print(summary)\n",
        "print(\"\\n--- Generated Abstract ---\")\n",
        "print(abstract)\n",
        "print(\"\\n--- Relevant References ---\")\n",
        "if references:\n",
        "    for ref in references:\n",
        "        print(ref)\n",
        "else:\n",
        "    print(\"No relevant references found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V37qkN4LEOyr"
      },
      "outputs": [],
      "source": [
        "# Add a padding token to the tokenizer if it doesn't have one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Padding token set for the tokenizer.\")\n",
        "\n",
        "# Re-run the example usage\n",
        "user_prompt = \"Summarize the key findings about data analysis in the provided documents.\"\n",
        "summary, abstract, references = generate_summary_abstract_and_references_with_prompt(user_prompt, model, tokenizer, text_chunks)\n",
        "\n",
        "print(\"--- Generated Summary ---\")\n",
        "print(summary)\n",
        "print(\"\\n--- Generated Abstract ---\")\n",
        "print(abstract)\n",
        "print(\"\\n--- Relevant References ---\")\n",
        "if references:\n",
        "    for ref in references:\n",
        "        print(ref)\n",
        "else:\n",
        "    print(\"No relevant references found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38977298"
      },
      "source": [
        "## Evaluate the model (optional)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the fine-tuned model on a separate dataset to assess the quality of the generated summaries and abstracts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f6c032b"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a small evaluation dataset\n",
        "# We will take a few chunks and manually create reference summaries/abstracts\n",
        "evaluation_data = []\n",
        "\n",
        "# Take the first 3 chunks as an example\n",
        "for i in range(3):\n",
        "    original_text = text_chunks[i]\n",
        "    # Manually create a simple reference summary and abstract for each chunk\n",
        "    # In a real scenario, this would require careful manual annotation\n",
        "    reference_summary = f\"This is a manually created summary for chunk {i+1}.\"\n",
        "    reference_abstract = f\"This is a manually created abstract for chunk {i+1}.\"\n",
        "\n",
        "    evaluation_data.append({\n",
        "        \"original_text\": original_text,\n",
        "        \"reference_summary\": reference_summary,\n",
        "        \"reference_abstract\": reference_abstract\n",
        "    })\n",
        "\n",
        "print(f\"Created evaluation dataset with {len(evaluation_data)} entries.\")\n",
        "\n",
        "# Step 2: Generate summaries and abstracts using the fine-tuned model\n",
        "model_outputs = []\n",
        "\n",
        "\n",
        "for item in evaluation_data:\n",
        "    prompt = \"Summarize and abstract the following text:\\n\" + item[\"original_text\"]\n",
        "    generated_summary, generated_abstract, generated_references = generate_summary_abstract_and_references_with_prompt(\n",
        "        prompt, model, tokenizer, text_chunks\n",
        "    )\n",
        "    model_outputs.append({\n",
        "        \"original_text\": item[\"original_text\"],\n",
        "        \"reference_summary\": item[\"reference_summary\"],\n",
        "        \"reference_abstract\": item[\"reference_abstract\"],\n",
        "        \"generated_summary\": generated_summary,\n",
        "        \"generated_abstract\": generated_abstract,\n",
        "        \"generated_references\": generated_references\n",
        "    })\n",
        "\n",
        "print(f\"Generated summaries and abstracts for {len(model_outputs)} entries.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a8a0ca7"
      },
      "outputs": [],
      "source": [
        "# Step 3 & 4: Implement and utilize ROUGE evaluation metrics\n",
        "# Install the rouge package if not already installed\n",
        "!pip install rouge_score\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialize the ROUGE scorer\n",
        "# Use different metrics: rouge1, rouge2, and rougeL\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "rouge_scores = []\n",
        "\n",
        "# Calculate ROUGE scores for each generated summary/abstract pair\n",
        "for output in model_outputs:\n",
        "    reference_summary = output[\"reference_summary\"]\n",
        "    generated_summary = output[\"generated_summary\"]\n",
        "    reference_abstract = output[\"reference_abstract\"]\n",
        "    generated_abstract = output[\"generated_abstract\"]\n",
        "\n",
        "    # Calculate scores for summary\n",
        "    summary_scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "    # Calculate scores for abstract\n",
        "    abstract_scores = scorer.score(reference_abstract, generated_abstract)\n",
        "\n",
        "    rouge_scores.append({\n",
        "        \"summary_rouge1\": summary_scores[\"rouge1\"].fmeasure,\n",
        "        \"summary_rouge2\": summary_scores[\"rouge2\"].fmeasure,\n",
        "        \"summary_rougel\": summary_scores[\"rougeL\"].fmeasure,\n",
        "        \"abstract_rouge1\": abstract_scores[\"rouge1\"].fmeasure,\n",
        "        \"abstract_rouge2\": abstract_scores[\"rouge2\"].fmeasure,\n",
        "        \"abstract_rougel\": abstract_scores[\"rougeL\"].fmeasure,\n",
        "    })\n",
        "\n",
        "# Calculate average ROUGE scores\n",
        "avg_summary_rouge1 = sum([score[\"summary_rouge1\"] for score in rouge_scores]) / len(rouge_scores)\n",
        "avg_summary_rouge2 = sum([score[\"summary_rouge2\"] for score in rouge_scores]) / len(rouge_scores)\n",
        "avg_summary_rougel = sum([score[\"summary_rougel\"] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "avg_abstract_rouge1 = sum([score[\"abstract_rouge1\"] for score in rouge_scores]) / len(rouge_scores)\n",
        "avg_abstract_rouge2 = sum([score[\"abstract_rouge2\"] for score in rouge_scores]) / len(rouge_scores)\n",
        "avg_abstract_rougel = sum([score[\"abstract_rougel\"] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "print(\"\\n--- Average ROUGE Scores ---\")\n",
        "print(f\"Average Summary ROUGE-1: {avg_summary_rouge1:.4f}\")\n",
        "print(f\"Average Summary ROUGE-2: {avg_summary_rouge2:.4f}\")\n",
        "print(f\"Average Summary ROUGE-L: {avg_summary_rougel:.4f}\")\n",
        "print(f\"Average Abstract ROUGE-1: {avg_abstract_rouge1:.4f}\")\n",
        "print(f\"Average Abstract ROUGE-2: {avg_abstract_rouge2:.4f}\")\n",
        "print(f\"Average Abstract ROUGE-L: {avg_abstract_rougel:.4f}\")\n",
        "\n",
        "# Step 5: Qualitative evaluation (manual review)\n",
        "print(\"\\n--- Qualitative Evaluation (Sample Review) ---\")\n",
        "# Print a sample of generated vs. reference summaries/abstracts for manual review\n",
        "for i, output in enumerate(model_outputs):\n",
        "    print(f\"\\n--- Entry {i+1} ---\")\n",
        "    print(\"Reference Summary:\")\n",
        "    print(output['reference_summary'])\n",
        "    print(\"Generated Summary:\")\n",
        "    print(output['generated_summary'])\n",
        "    print(\"Reference Abstract:\")\n",
        "    print(output['reference_abstract'])\n",
        "    print(\"Generated Abstract:\")\n",
        "    print(output['generated_abstract'])\n",
        "    print(\"Generated References:\")\n",
        "    if output['generated_references']:\n",
        "        for ref in output['generated_references']:\n",
        "            print(ref)\n",
        "    else:\n",
        "        print(\"No relevant references found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CODE AS ONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28e7dec5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Folder containing PDFs\n",
        "pdf_folder = \"/content/drive/MyDrive/Colab Notebooks/pdf2\"\n",
        "\n",
        "# List to store extracted text with source information\n",
        "extracted_data = []\n",
        "\n",
        "# Loop over all PDF files in the folder\n",
        "for filename in os.listdir(pdf_folder):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_folder, filename)\n",
        "\n",
        "        # Open and extract text page by page\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page_num in range(doc.page_count):\n",
        "                page = doc.load_page(page_num)\n",
        "                text = page.get_text()\n",
        "                extracted_data.append({\n",
        "                    \"text\": text,\n",
        "                    \"filename\": filename,\n",
        "                    \"page_num\": page_num + 1  # Page numbers are 1-based\n",
        "                })\n",
        "\n",
        "print(f\"Extracted text from {len(extracted_data)} pages across all PDFs.\")\n",
        "\n",
        "# Now, create chunks from the extracted data, preserving source information\n",
        "chunk_size = 1000\n",
        "text_chunks_with_info = []\n",
        "current_chunk_text = \"\"\n",
        "current_chunk_start_info = None\n",
        "\n",
        "for item in extracted_data:\n",
        "    text = item[\"text\"]\n",
        "    filename = item[\"filename\"]\n",
        "    page_num = item[\"page_num\"]\n",
        "\n",
        "    # If this is the start of a new chunk accumulation, record the source info\n",
        "    if not current_chunk_start_info:\n",
        "        current_chunk_start_info = {\"filename\": filename, \"start_page\": page_num}\n",
        "\n",
        "    current_chunk_text += text\n",
        "\n",
        "    # If the current chunk text is long enough or this is the last item, create a chunk\n",
        "    while len(current_chunk_text) >= chunk_size:\n",
        "        text_chunks_with_info.append({\n",
        "            \"text\": current_chunk_text[:chunk_size],\n",
        "            \"filename\": current_chunk_start_info[\"filename\"],\n",
        "            \"start_page\": current_chunk_start_info[\"start_page\"]\n",
        "        })\n",
        "        current_chunk_text = current_chunk_text[chunk_size:]\n",
        "        # Update start page for the next potential chunk from the same page\n",
        "        if len(current_chunk_text) > 0:\n",
        "             current_chunk_start_info = {\"filename\": filename, \"start_page\": page_num}\n",
        "        else:\n",
        "             current_chunk_start_info = None # Reset if the current page is fully consumed\n",
        "\n",
        "\n",
        "# Add any remaining text as the last chunk\n",
        "if len(current_chunk_text) > 0:\n",
        "     text_chunks_with_info.append({\n",
        "            \"text\": current_chunk_text,\n",
        "            \"filename\": current_chunk_start_info[\"filename\"],\n",
        "            \"start_page\": current_chunk_start_info[\"start_page\"]\n",
        "        })\n",
        "\n",
        "\n",
        "print(f\"Created {len(text_chunks_with_info)} text chunks with source information.\")\n",
        "print(\"First chunk with info:\")\n",
        "print(text_chunks_with_info[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8ee12ee"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_summary_abstract_and_references(generated_text, text_chunks_with_info):\n",
        "    \"\"\"\n",
        "    Extracts summary and abstract from generated text and finds relevant references\n",
        "    with filename and page number information.\n",
        "\n",
        "    Args:\n",
        "        generated_text (str): The text generated by the LLM.\n",
        "        text_chunks_with_info (list): A list of dictionaries, where each dictionary\n",
        "                                     contains 'text', 'filename', and 'start_page'.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the extracted summary (str), abstract (str),\n",
        "               and relevant references (list of str).\n",
        "    \"\"\"\n",
        "    summary = \"\"\n",
        "    abstract = \"\"\n",
        "    references = []\n",
        "\n",
        "    # Simple approach: Look for patterns indicating summary and abstract\n",
        "    # This will heavily depend on how the model was fine-tuned and the nature of the text\n",
        "    summary_match = re.search(r\"Summary:\\s*(.*?)(?:Abstract:|$)\", generated_text, re.DOTALL | re.IGNORECASE)\n",
        "    if summary_match:\n",
        "        summary = summary_match.group(1).strip()\n",
        "\n",
        "    abstract_match = re.search(r\"Abstract:\\s*(.*?)(?:Summary:|$)\", generated_text, re.DOTALL | re.IGNORECASE)\n",
        "    if abstract_match:\n",
        "        abstract = abstract_match.group(1).strip()\n",
        "\n",
        "    # If specific patterns are not found, take the beginning of the text as summary/abstract\n",
        "    if not summary and not abstract:\n",
        "        # Simple split, might need adjustment based on actual output format\n",
        "        split_text = generated_text.split('\\n\\n', 1)\n",
        "        abstract = split_text[0].strip()\n",
        "        if len(split_text) > 1:\n",
        "            summary = split_text[1].strip()\n",
        "        else:\n",
        "            summary = abstract # Or handle as needed\n",
        "\n",
        "    # Find relevant references by searching for keywords/phrases in original chunks\n",
        "    keywords = set()\n",
        "    # Use first few words of summary and abstract as keywords\n",
        "    keywords.update(summary.split()[:10])\n",
        "    keywords.update(abstract.split()[:10])\n",
        "\n",
        "    relevant_chunks = []\n",
        "    for chunk_info in text_chunks_with_info:\n",
        "        chunk_text = chunk_info[\"text\"]\n",
        "        filename = chunk_info[\"filename\"]\n",
        "        start_page = chunk_info[\"start_page\"]\n",
        "\n",
        "        if any(re.search(r'\\b' + re.escape(keyword) + r'\\b', chunk_text, re.IGNORECASE) for keyword in keywords if len(keyword) > 3): # Check for keywords with length > 3\n",
        "            # Limit the snippet length for better readability\n",
        "            snippet = chunk_text[:500] + \"...\" if len(chunk_text) > 500 else chunk_text\n",
        "            relevant_chunks.append(f\"Reference from {filename} (page {start_page}): {snippet}\")\n",
        "\n",
        "    references = relevant_chunks\n",
        "\n",
        "    return summary, abstract, references\n",
        "\n",
        "def generate_summary_abstract_and_references_with_prompt(prompt, model, tokenizer, text_chunks_with_info, max_length=512, num_return_sequences=1, no_repeat_ngram_size=2, do_sample=True, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generates summary, abstract, and references for a given prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The user prompt.\n",
        "        model: The fine-tuned Hugging Face model.\n",
        "        tokenizer: The corresponding tokenizer.\n",
        "        text_chunks_with_info (list): A list of dictionaries, where each dictionary\n",
        "                                     contains 'text', 'filename', and 'start_page'.\n",
        "        max_length (int): The maximum length of the generated text.\n",
        "        num_return_sequences (int): The number of sequences to generate.\n",
        "        no_repeat_ngram_size (int): The size of ngrams that should not be repeated.\n",
        "        do_sample (bool): Whether to use sampling for generation.\n",
        "        temperature (float): Controls the randomness in sampling.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the extracted summary (str), abstract (str),\n",
        "               and relevant references (list of str).\n",
        "    \"\"\"\n",
        "    generated_text = generate_summary_and_abstract(\n",
        "        prompt,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    summary, abstract, references = extract_summary_abstract_and_references(generated_text, text_chunks_with_info)\n",
        "\n",
        "    return summary, abstract, references\n",
        "\n",
        "# Example usage:\n",
        "user_prompt = \"Summarize the key findings about data analysis in the provided documents.\"\n",
        "# Call the updated function with text_chunks_with_info\n",
        "summary, abstract, references = generate_summary_abstract_and_references_with_prompt(user_prompt, model, tokenizer, text_chunks_with_info)\n",
        "\n",
        "print(\"--- Generated Summary ---\")\n",
        "print(summary)\n",
        "print(\"\\n--- Generated Abstract ---\")\n",
        "print(abstract)\n",
        "print(\"\\n--- Relevant References ---\")\n",
        "if references:\n",
        "    for ref in references:\n",
        "        print(ref)\n",
        "else:\n",
        "    print(\"No relevant references found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35e30dc6"
      },
      "outputs": [],
      "source": [
        "# Re-run the example usage to verify the updated reference format\n",
        "user_prompt = \"Summarize the key findings about data analysis in the provided documents.\"\n",
        "summary, abstract, references = generate_summary_abstract_and_references_with_prompt(user_prompt, model, tokenizer, text_chunks_with_info)\n",
        "\n",
        "print(\"--- Generated Summary ---\")\n",
        "print(summary)\n",
        "print(\"\\n--- Generated Abstract ---\")\n",
        "print(abstract)\n",
        "print(\"\\n--- Relevant References ---\")\n",
        "if references:\n",
        "    for ref in references:\n",
        "        print(ref)\n",
        "else:\n",
        "    print(\"No relevant references found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e3da453"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial text extraction process was successfully modified to store the source filename and page number for each extracted text chunk, resulting in 2140 chunks from 831 pages.\n",
        "*   The `extract_summary_abstract_and_references` function was updated to utilize the stored source information, enabling it to generate references that include the filename and starting page number.\n",
        "*   Testing confirmed that the updated reference extraction function correctly displays references with the filename and page number format.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current reference extraction method is based on simple keyword matching; consider implementing a more sophisticated approach, such as semantic similarity search, to find more relevant references.\n",
        "*   The model's ability to generate accurate summaries, abstracts, and identify relevant references depends heavily on its training data and fine-tuning process, which were not detailed here. Future work could involve fine-tuning the model specifically for this task on a dataset of research papers and their corresponding summaries, abstracts, and reference sections.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
